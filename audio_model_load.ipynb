{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbtTqUx90woB",
        "outputId": "7d5064fd-306e-4b29-fb9e-ed00450a2b36"
      },
      "outputs": [],
      "source": [
        "#All Downloads go here\n",
        "# !wget https://cdn.discordapp.com/attachments/750937322299588660/1107618190843990036/medyo.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi-8qusX1dFV",
        "outputId": "6ea22015-f974-4ef9-b37f-5b0c5e12b641"
      },
      "outputs": [],
      "source": [
        "#!pip install SpeechRecognition pydub pocketsphinx\n",
        "# %pip install pydub\n",
        "#!pip install ffmpeg-python\n",
        "# %pip install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGF6wWhUDy9K"
      },
      "outputs": [],
      "source": [
        "#All Imports go here\n",
        "# from IPython.display import Audio, display, HTML\n",
        "import scipy\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from google.colab.output import eval_js\n",
        "# from base64 import b64decode\n",
        "# from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3c4wjCB2V12"
      },
      "outputs": [],
      "source": [
        "# Set up variables for MFCCs\n",
        "# words = ['artista', 'sa gawas sa nasud', 'aborsyon', 'katabang', 'hapon', 'sa taas', 'magtatambag', 'nahadlok', 'makaapekto', 'ipahiangay', 'biyaan', 'AIDS', 'kaniadto', 'pagkahuman', 'tambag', 'African-American', 'alyansa', 'makapalit', 'hamtong na', 'kalihokan', 'tinuud', 'tabok', 'abante', 'hingpit nga', 'kaalyado', 'makab-ot', 'moila', 'sa unahan', 'airport', 'aktibista', 'dugangi', 'aktibo', 'moangkon', 'administrasyon', 'edad', 'buhi', 'ayroplano', 'lihok', 'advertising', 'sa tinuud', 'panguma', 'hingpit', 'mouyon', 'advance', 'batok', 'aksidente', 'tumong', 'pag-abuso', 'tabang', 'pagkab-ot', 'ubanan', 'mga nakab-ot', 'Africa', 'mosuhop', 'pagsagop', 'bentaha', 'tin-edyer', 'ahente', 'sa usa ka', 'tigdumala', 'eroplano', 'pagdugang', 'access', 'alkohol', 'sumala', 'pag-usab', 'asoy', 'pagpahiangay', 'makuha', 'manlalaban', 'tanan', 'dugang', 'abilidad', 'dawata', 'ahensya', 'pagdawat', 'agresibo', 'album', 'kasabutan', 'adjust', 'agenda', 'tukma', 'hapit', 'makahimo', 'tugoti', 'asido', 'sumbong', 'ad', 'panimpalad', 'adres', 'akademiko', 'tambagi', 'igo', 'pagkawala', 'aksyon', 'hangin', 'nakadayeg', 'mahitungod sa']\n",
        "words = ['langit', 'butong', 'ibabaw', 'makapalit', 'lima', 'duha', 'wala', 'sukad', 'asa', 'humok', 'miingon', 'inahan', 'raman', 'dugo', 'ko', 'usba', 'hinungdan', 'busog', 'kasuko', 'giingon', 'saging', 'ulo', 'kaniya', 'tara', 'man', 'pagsulti', 'bati', 'ta', 'kamot', 'uo', 'buntag', 'lugar', 'tiya', 'laay', 'natulog', 'ilaha', 'usab', 'tatay', 'ingato', 'diin', 'basta', 'dugang', 'namo', 'unsa', 'hagad', 'bago', 'tiil', 'unta', 'ate', 'hapon', 'gahapon', 'kasabot', 'dapit', 'kinsa', 'pirmi', 'pito', 'kato', 'asawa', 'tubig', 'nanay', 'iyaha', 'dili', 'hubog', 'kana', 'maulaw', 'mao', 'dako', 'tawo', 'gutom', 'pay', 'hinuon', 'nasod', 'hilantan', 'kani', 'kuya', 'puyo', 'sakyan', 'yuta', 'mangga', 'gwapo', 'aron', 'pila', 'gawas', 'gibuhat', 'buhi', 'pakwan', 'babaye', 'sala', 'kanang', 'siya', 'amahan', 'kwarta', 'amoa', 'nabuang', 'tuo', 'sama', 'kinahanglan', 'sambag', 'kini', 'maayong', 'gani', 'salamat', 'ikaw', 'tiyo', 'gabii', 'bana', 'human', 'gamay', 'ugma', 'gwapa', 'dugay', 'pangutana', 'bukid', 'na', 'mansanas', 'kapayas', 'agi', 'bitaw', 'palihug', 'ako', 'tuig', 'tiguwang', 'buot', 'gahi', 'apan', 'bata', 'ngadto', 'oi', 'pasayloa', 'kita', 'nila', 'ah', 'lalaki', 'libo', 'didto', 'dalan', 'nahurot', 'pero', 'natagak', 'pwede', 'buwan', 'ligo', 'trabaho', 'tambis', 'upat', 'usa', 'maoy', 'ngalan', 'unom', 'balay', 'patay', 'niini', 'among', 'napu', 'amping', 'tungod', 'pinoy', 'ninyo', 'gyod', 'atis', 'unsaon', 'basin', 'kanusa', 'dungan', 'ana', 'abokado', 'atubangan', 'walo', 'may', 'tag', 'tibuok', 'siyam', 'naa', 'istorya', 'nawung', 'balita', 'lingaw', 'naman', 'bantayan', 'naligo', 'bayabas', 'buungon', 'manghuwam', 'luyo', 'unya', 'lang', 'ubos', 'sobra', 'pinaagi', 'tulo', 'kanato', 'nawong', 'hasta', 'lugara', 'buang', 'ang', 'sulod', 'edad', 'balaod', 'iya']\n",
        "NUM_MFCCS = 13\n",
        "MAX_LEN = 192\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 1\n",
        "HOP_LENGTH = 512\n",
        "NUM_CLASSES = len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eof9ddMc0VWA",
        "outputId": "c8dd270a-76d7-4bf8-f5a1-84cd9207ebf0"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('weights.h5')\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAXqVMvv8WEE"
      },
      "outputs": [],
      "source": [
        "def pad_mfcc(mfcc, max_len):\n",
        "    if mfcc.shape[1] > max_len:\n",
        "        # Trim the MFCCs if the length exceeds the maximum length\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "        print(\"Max Length\")\n",
        "    else:\n",
        "        # Pad the MFCCs with zeros if the length is less than the maximum length\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    return mfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNO2uSLn3Zbh"
      },
      "outputs": [],
      "source": [
        "# Define a function to normalize a chunk to a target amplitude.\n",
        "def match_target_amplitude(aChunk, target_dBFS):\n",
        "    ''' Normalize given audio chunk '''\n",
        "    change_in_dBFS = target_dBFS - aChunk.dBFS\n",
        "    return aChunk.apply_gain(change_in_dBFS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eMwPvqeZzimu"
      },
      "outputs": [],
      "source": [
        "def create_chunks(sound_path):\n",
        "  sound = AudioSegment.from_file(sound_path)\n",
        "  chunks = split_on_silence (\n",
        "      # Use the loaded audio.\n",
        "      sound, \n",
        "      # Specify that a silent chunk must be at least 300 milliseconds\n",
        "      min_silence_len = 150,\n",
        "      # Consider a chunk silent if it's quieter than 10 dBFS.\n",
        "      silence_thresh = sound.dBFS - 16,\n",
        "      keep_silence = 250\n",
        "  )\n",
        "  # Process each chunk with your parameters\n",
        "  for i, chunk in enumerate(chunks):\n",
        "      # Create a silence chunk that's 0.5 seconds (or 500 ms) long for padding.\n",
        "      silence_chunk = AudioSegment.silent(duration=00)\n",
        "      # Add the padding chunk to beginning and end of the entire chunk.\n",
        "      audio_chunk = silence_chunk + chunk + silence_chunk\n",
        "      # Normalize the entire chunk.\n",
        "      normalized_chunk = match_target_amplitude(audio_chunk, -25.0)\n",
        "      # Export the audio chunk with new bitrate.\n",
        "      print(\"Exporting chunk{0}.wav.\".format(i))\n",
        "      normalized_chunk.export(\n",
        "          \".//chunk{0}.wav\".format(i),\n",
        "          bitrate = \"256k\",\n",
        "          format = \"wav\"\n",
        "      )\n",
        "      !ffmpeg -i \"chunk{i}.wav\" -af silenceremove=1:0:-33dB -ac 1 -ar 16000 -vn \"cleaned-chunk{i}.wav\" -y -loglevel panic\n",
        "      #os.system(f'ffmpeg -i \"chunk'+ str(i) +'.wav\" -af silenceremove=1:0:-33dB -ac 1 -ar 16000 -vn \"cleaned/chunk'+ str(i) +'\" -y')\n",
        "  return len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGKsOxyUI2DP"
      },
      "outputs": [],
      "source": [
        "#this is to test the segmentation\n",
        "def test_chunks(file_path):\n",
        "\n",
        "  count = create_chunks(file_path)\n",
        "  predictions = []\n",
        "\n",
        "  for i in range(count):\n",
        "    audio_segment, sr = librosa.load(\"cleaned-chunk{0}.wav\".format(i), sr=SAMPLE_RATE)\n",
        "    mfccs = librosa.feature.mfcc(y=audio_segment, sr=sr, n_mfcc=NUM_MFCCS)\n",
        "    mfcc_padded = pad_mfcc(mfccs, MAX_LEN)\n",
        "    prediction = model.predict(mfcc_padded.reshape(1, NUM_MFCCS, MAX_LEN, 1))\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    predictions.append(predicted_class)\n",
        "\n",
        "  predicted_words = [words[pred] for pred in predictions]\n",
        "  text = ''\n",
        "  for word in predicted_words:\n",
        "      text += word + ' '\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RlEya7JI7mJ"
      },
      "outputs": [],
      "source": [
        "#this is for testing singular words\n",
        "def test_single(test_path):\n",
        "  test_audio, sr = librosa.load(test_path, sr=SAMPLE_RATE)\n",
        "  test_mfcc = librosa.feature.mfcc(y=test_audio, sr=sr, n_mfcc=NUM_MFCCS)\n",
        "  q = model.predict(pad_mfcc(test_mfcc, MAX_LEN).reshape(1, NUM_MFCCS, MAX_LEN, 1))\n",
        "  print(words[np.argmax(q)])\n",
        "  print(np.argmax(q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exporting chunk0.wav.\n",
            "Exporting chunk1.wav.\n",
            "Exporting chunk2.wav.\n",
            "Exporting chunk3.wav.\n",
            "Exporting chunk4.wav.\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "ta duha ulo ta sama \n",
            "removed 'chunk0.wav'\n",
            "removed 'chunk1.wav'\n",
            "removed 'chunk2.wav'\n",
            "removed 'chunk3.wav'\n",
            "removed 'chunk4.wav'\n"
          ]
        }
      ],
      "source": [
        "!rm.exe cleaned-chunk*.wav\n",
        "test_chunks(\"ay_lmao.wav\")\n",
        "!rm.exe -v chunk*.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "import google_api as ga\n",
        "import gpt_api_tagalog as gt\n",
        "\n",
        "def record_audio(segment_duration=1.0, silence_threshold=30):\n",
        "    # Configure audio recording parameters\n",
        "    sample_rate = 44100  # Sample rate (Hz)\n",
        "\n",
        "    # Initialize variables\n",
        "    segment = np.array([], dtype=np.float32)\n",
        "    silent_duration = 40\n",
        "    silent_frames = 0\n",
        "    state = 0\n",
        "    def audio_callback(indata, frames, time, status):\n",
        "        nonlocal segment, silent_frames, state\n",
        "\n",
        "        volume_norm = np.linalg.norm(indata)*10\n",
        "        # print(\"volume: {0} || silent_frames: {1}\".format(int(volume_norm), silent_frames))\n",
        "        # print(rms)\n",
        "\n",
        "        if volume_norm < silence_threshold:\n",
        "            silent_frames += 1\n",
        "            if state == 0:\n",
        "                segment = np.array([], dtype=np.float32)\n",
        "        else:\n",
        "            silent_frames = 0\n",
        "            state = 1\n",
        "\n",
        "        # Append the current audio segment to the list if it contains enough samples\n",
        "        segment = np.concatenate([segment, indata[:, 0]])\n",
        "        if silent_frames >= silent_duration and state == 1:\n",
        "            segment = float2pcm(segment)\n",
        "            save_audio_segment(segment)\n",
        "            # print(\"Segment Saved\")\n",
        "            state = 0\n",
        "            segment = np.array([], dtype=np.float32)\n",
        "\n",
        "    # Start audio recording\n",
        "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=sample_rate, device=1):\n",
        "        print(\"Recording audio... Press Ctrl+C to stop.\")\n",
        "        while True:\n",
        "            try:\n",
        "                sd.sleep(100)\n",
        "                if silent_frames >= sample_rate * segment_duration:\n",
        "                    break\n",
        "            except KeyboardInterrupt:\n",
        "                break\n",
        "    return\n",
        "\n",
        "def save_audio_segment(segment):\n",
        "    filename = f\"D:\\\\subtlr\\\\ay_lmao.wav\"\n",
        "    wavfile.write(filename, 44100, segment)\n",
        "    try:\n",
        "        transcript = ga.transcribe(\"D:\\\\subtlr\\\\ay_lmao.wav\")\n",
        "        print(\"Transcript: {}\".format(gt.translate(transcript)))\n",
        "        test_chunks(\"ay_lmao.wav\")\n",
        "    except:\n",
        "        pass\n",
        "    finally:\n",
        "        pass\n",
        "        # !rm.exe -v chunk*.wav\n",
        "        # !rm.exe cleaned-chunk*.wav\n",
        "\n",
        "\n",
        "def float2pcm(sig, dtype='int16'):\n",
        "    sig = np.asarray(sig)\n",
        "    dtype = np.dtype(dtype)\n",
        "    i = np.iinfo(dtype)\n",
        "    abs_max = 2 ** (i.bits - 1)\n",
        "    offset = i.min + abs_max\n",
        "    return (sig * abs_max + offset).clip(i.min, i.max).astype(dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording audio... Press Ctrl+C to stop.\n",
            "Recorded Text: duha tulo\n",
            "Transcript:  Two three.\n",
            "Exporting chunk0.wav.\n",
            "Exporting chunk1.wav.\n",
            "Exporting chunk2.wav.\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "ta gahapon kamot \n"
          ]
        }
      ],
      "source": [
        "record_audio()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
